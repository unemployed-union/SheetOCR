import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# --- [1] ëª¨ë¸ ì •ì˜ (ìš°ë¦¬ê°€ ë§Œë“  Hybrid êµ¬ì¡°) ---
class HybridEmbed(nn.Module):
    def __init__(self, in_channels=3, embed_dim=768):
        super().__init__()
        # CNN Backbone
        self.cnn = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, 2, 1),  # /2
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.Conv2d(32, 64, 3, 2, 1),           # /4
            nn.BatchNorm2d(64), nn.ReLU(),
            # ë†’ì´ë§Œ ì¤„ì´ê³  ë„ˆë¹„(ì‹œê°„)ëŠ” ìœ ì§€
            nn.Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0)), 
            nn.BatchNorm2d(128), nn.ReLU(),
        )
        # 128ì±„ë„ * 14ë†’ì´ = 1792
        self.proj = nn.Linear(128 * 14, embed_dim)

    def forward(self, x):
        # x: (Batch, 3, 112, 448)
        x = self.cnn(x)  # -> (Batch, 128, 14, 112) [C, H, W]
        
        # [í•µì‹¬ ë””ë²„ê¹…] ì°¨ì› í™•ì¸
        # ìœ„ì—ëŠ” H, ì•„ë˜ëŠ” ê¸€ì ë¨¸ë¦¬. ì´ ì •ë³´ë¥¼ ì„¸ë¡œë¡œ í•©ì¹¨
        # (Batch, C, H, W) -> (Batch, W, C, H) -> (Batch, W, C*H)
        x = x.permute(0, 3, 1, 2) 
        x = x.flatten(2) # (Batch, 112, 1792)
        x = self.proj(x) # (Batch, 112, 768)
        return x

# [ìˆ˜ì •ëœ ëª¨ë¸] íŠ¸ëœìŠ¤í¬ë¨¸ ì œê±°, CNN ì§ê²°
class SimpleViTForOCR(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # 1. HybridEmbed (CNN)
        self.embed = HybridEmbed(embed_dim=768)
        
        # 2. [ìˆ˜ì •] Transformer ì œê±°! 
        # ë³µì¡í•œ ì—°ì‚° ì—†ì´ CNN ì¶œë ¥ì„ ë°”ë¡œ ë¶„ë¥˜ê¸°ì— ë„£ìŠµë‹ˆë‹¤.
        # self.pos_embed = ... (ì‚­ì œ)
        # self.encoder = ... (ì‚­ì œ)
        
        # 3. Head (ë¶„ë¥˜ê¸°)
        self.head = nn.Linear(768, vocab_size)

    def forward(self, x):
        # ì…ë ¥ -> CNN(HybridEmbed) -> (Batch, 112, 768)
        x = self.embed(x)
        
        # Transformer ì—†ì´ ë°”ë¡œ ì˜ˆì¸¡
        # x = x + self.pos_embed (ì‚­ì œ)
        # x = self.encoder(x) (ì‚­ì œ)
        
        return self.head(x)

# --- [2] ê°€ì§œ ë°ì´í„° ìƒì„± (ê³ ì •ëœ íŒ¨í„´) ---
# ë°°ì¹˜ 2ê°œ, ì´ë¯¸ì§€ í¬ê¸° 112x448
dummy_images = torch.randn(2, 3, 112, 448) 

# ì •ë‹µ: 1ë²ˆì€ "ABC"(1,2,3), 2ë²ˆì€ "A"(1) ë¼ê³  ê°€ì •
# Target Lengths: ì²«ë²ˆì§¸ëŠ” 3ê¸€ì, ë‘ë²ˆì§¸ëŠ” 1ê¸€ì
target_lengths = torch.tensor([3, 1], dtype=torch.long)
targets = torch.tensor([1, 2, 3, 1], dtype=torch.long) # ë‹¤ ì´ì–´ë¶™ì„

# --- [3] í•™ìŠµ ë£¨í”„ (ì˜¤ë²„í• í…ŒìŠ¤íŠ¸) ---
# Vocab Size: 0(Blank) + 1,2,3(ê¸€ì) = 4ê°œ
model = SimpleViTForOCR(vocab_size=5) 
optimizer = optim.AdamW(model.parameters(), lr=1e-3)
criterion = nn.CTCLoss(blank=0, zero_infinity=True)

print("ğŸš€ ì‚°ì†Œí˜¸í¡ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
model.train()

for epoch in range(50):
    optimizer.zero_grad()
    
    outputs = model(dummy_images) # (Batch, 112, Vocab)
    
    # CTC Loss ì…ë ¥ í˜•íƒœ: (Time, Batch, Vocab)
    outputs = outputs.permute(1, 0, 2)
    log_probs = nn.functional.log_softmax(outputs, dim=2)
    
    # Input Lengths: ëª¨ë¸ì´ ë±‰ì€ ì‹œê°„ ê¸¸ì´ (112)
    input_lengths = torch.full(size=(2,), fill_value=112, dtype=torch.long)
    
    loss = criterion(log_probs, targets, input_lengths, target_lengths)
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

# --- [4] ì˜ˆì¸¡ í™•ì¸ ---
print("\n[ê²°ê³¼ í™•ì¸]")
with torch.no_grad():
    model.eval()
    outputs = model(dummy_images)
    pred = outputs.argmax(dim=2) # (Batch, 112)
    print(f"ì˜ˆì¸¡ê°’(ì¸ë±ìŠ¤) 0ë²ˆ ìƒ˜í”Œ ì•ë¶€ë¶„: {pred[0, :10].tolist()}")
    # ì •ë‹µ 1, 2, 3ì´ ë³´ì—¬ì•¼ í•¨ (ì¤‘ê°„ì— 0ì´ ì„ì—¬ ìˆì–´ë„ ë¨)