import os
import torch
import numpy as np
from torch.utils.data import Dataset
from PIL import Image
from tqdm import tqdm

class SheetMusicDataset(Dataset):
    def __init__(self, root_dir, df, tokenizer, transform=None):
        self.root_dir = root_dir
        self.df = df
        self.tokenizer = tokenizer
        self.transform = transform # ì •ê·œí™”(Normalize)ë§Œ ë‚¨ê¹€
        
        self.cached_tensors = [] # ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë¼ 'í…ì„œ'ë¥¼ ì €ì¥
        self.cached_targets = [] # ì •ë‹µë„ ë¯¸ë¦¬ í…ì„œë¡œ ë³€í™˜í•´ì„œ ì €ì¥
        
        print(f"ğŸ”„ ë°ì´í„° {len(df)}ì¥ í…ì„œ ë³€í™˜ ë° RAM ìºì‹± ì¤‘... (ìµœì í™”)")
        
        resize_tool = Image.BICUBIC
        target_size = (448, 112)
        
        for idx in tqdm(range(len(df))):
            try:
                # 1. ì´ë¯¸ì§€ ë¡œë“œ & ë¦¬ì‚¬ì´ì¦ˆ
                file_name = df.iloc[idx]['file_name']
                img_path = os.path.join(self.root_dir, file_name)
                img = Image.open(img_path).convert("L")
                img = img.resize(target_size, resample=resize_tool)
                
                # 2. [í•µì‹¬] ë°”ë¡œ í…ì„œ(UInt8)ë¡œ ë³€í™˜í•´ ì €ì¥!
                # transforms.ToTensor()ë¥¼ ì•ˆ ì“°ê³  numpyë¡œ ë°”ê¾¼ ë’¤ torchë¡œ ê°ìŒ‰ë‹ˆë‹¤.
                # (H, W, C) -> (C, H, W) ìˆœì„œ ë³€ê²½
                img_np = np.array(img)
                img_tensor = torch.from_numpy(img_np).unsqueeze(0) # dtype=torch.uint8 (ê°€ë²¼ì›€)

                self.cached_tensors.append(img_tensor)
                
                # 3. ì •ë‹µ(Target)ë„ ë¯¸ë¦¬ í…ì„œë¡œ ë³€í™˜
                text = df.iloc[idx]['text']
                if hasattr(self.tokenizer, 'encode'):
                    encoded = self.tokenizer.encode(text)
                else:
                    encoded = [self.tokenizer.token_to_id.get(c, 0) for c in text]
                self.cached_targets.append(torch.tensor(encoded, dtype=torch.long))
                
            except Exception as e:
                print(f"Error: {e}")
                # ì—ëŸ¬ ì‹œ ê²€ì€ìƒ‰ í…ì„œ ì¶”ê°€
                dummy = torch.zeros((3, 112, 448), dtype=torch.uint8)
                self.cached_tensors.append(dummy)
                self.cached_targets.append(torch.tensor([], dtype=torch.long))
                
        print("âœ… ìºì‹± ì™„ë£Œ! (CPU ë¶€í•˜ ìµœì†Œí™”)")

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # 1. ìºì‹±ëœ í…ì„œ êº¼ë‚´ê¸° (ì•„ì£¼ ë¹ ë¦„)
        # uint8 (0~255) ìƒíƒœ
        image_tensor = self.cached_tensors[idx]
        target = self.cached_targets[idx]
        
        # 2. Float ë³€í™˜ (0~1) : ë‚˜ëˆ„ê¸° ì—°ì‚°ë§Œ í•˜ë©´ ë¨
        # div(255)ëŠ” ë§¤ìš° ë¹ ë¦„
        image = image_tensor.float().div(255.0)
        
        # 3. Normalize ì ìš© (transformì— Normalizeë§Œ ìˆì–´ì•¼ í•¨)
        if self.transform:
            image = self.transform(image)

        return image, target

def collate_fn(batch):
    images, targets = zip(*batch)
    images = torch.stack(images, dim=0)
    targets = torch.cat(targets, dim=0)
    target_lengths = torch.tensor([len(t) for t in list(zip(*batch))[1]], dtype=torch.long)
    return images, targets, target_lengths