import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import pandas as pd

# ì‚¬ìš©ìžë‹˜ì˜ ëª¨ë“ˆ import
from .dataset import SheetMusicDataset, collate_fn
from .tokenizer import Tokenizer
from .vis_transformer import SimpleViTForOCR 

def debug_one_batch():
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ðŸ” ë””ë²„ê¹… ì‹œìž‘ (Device: {DEVICE})")

    # 1. í† í¬ë‚˜ì´ì € & ë°ì´í„° ë¡œë“œ
    vocab_list = [line.strip('\n') for line in open("vocab.txt", "r", encoding="utf-8")]
    tokenizer = Tokenizer(vocab_list)
    
    # [ì²´í¬ 1] 0ë²ˆì´ [PAD]ì¸ì§€ í™•ì¸
    print(f"ðŸ†” Vocab 0ë²ˆ ID í™•ì¸: '{vocab_list[0]}'")
    if vocab_list[0] != "[PAD]":
        print("ðŸš¨ [ê²½ê³ ] 0ë²ˆì´ [PAD]ê°€ ì•„ë‹™ë‹ˆë‹¤! CTC LossëŠ” 0ë²ˆì„ Blankë¡œ ì”ë‹ˆë‹¤.")

    # 2. ì•„ì£¼ ë‹¨ìˆœí•œ Transform (ì¦ê°• ë„ê³  ì •ê·œí™”ë§Œ)
    transform = transforms.Compose([
        transforms.Resize((112, 448)),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])

    # 3. ë°ì´í„° ë”± 4ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
    df = pd.read_json("dataset_vit/train_final/metadata.jsonl", lines=True).iloc[:4]
    dataset = SheetMusicDataset("dataset_vit/train_final", df, tokenizer, transform)
    loader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)

    # 4. ëª¨ë¸ ìƒì„± (ê¸°ì¡´ ì„¤ì • ê·¸ëŒ€ë¡œ)
    model = SimpleViTForOCR(
        vocab_size=tokenizer.get_vocab_size(),
        embed_dim=384, num_heads=6, num_layers=6 # í˜¹ì€ 12
    ).to(DEVICE)
    
    criterion = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0) # LR ë†’ê²Œ, ê·œì œ ë”

    # 5. ë°ì´í„° í•˜ë‚˜ë§Œ ë°˜ë³µ í•™ìŠµ (Overfitting)
    images, targets, target_lengths = next(iter(loader))
    images = images.to(DEVICE)
    targets = targets.to(DEVICE)
    target_lengths = target_lengths.to(DEVICE)

    # [ì²´í¬ 2] ìž…ë ¥ ê°’ ë²”ìœ„ í™•ì¸
    print(f"ðŸ“Š ìž…ë ¥ ì´ë¯¸ì§€ ë²”ìœ„: Min={images.min().item():.2f}, Max={images.max().item():.2f}")
    if images.max() > 200:
        print("ðŸš¨ [ì¹˜ëª…ì ] ì´ë¯¸ì§€ê°€ 0~255 ê°’ìž…ë‹ˆë‹¤! 0~1 í˜¹ì€ -1~1ë¡œ ì •ê·œí™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    model.train()
    print("\nðŸš€ í•™ìŠµ ì‹œìž‘ (100 Epoch ë™ì•ˆ 4ê°œë§Œ ì™¸ìš°ê¸°)...")
    
    for epoch in range(100):
        optimizer.zero_grad()
        outputs = model(images) # [Batch, Seq, Class]
        
        # CTC Loss ê³„ì‚°
        outputs_log_softmax = nn.functional.log_softmax(outputs, dim=2).permute(1, 0, 2)
        input_lengths = torch.full(size=(4,), fill_value=outputs.size(1), dtype=torch.long).to(DEVICE)
        
        loss = criterion(outputs_log_softmax, targets, input_lengths, target_lengths)
        loss.backward()
        optimizer.step()

        if (epoch+1) % 10 == 0:
            # ì˜ˆì¸¡ ê²°ê³¼ ë””ì½”ë”©í•´ì„œ ë³´ì—¬ì£¼ê¸°
            pred_idx = outputs.argmax(dim=2)[0].tolist() # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ
            pred_str = tokenizer.decode(pred_idx)
            
            # ì •ë‹µ ë¬¸ìžì—´
            target_str = tokenizer.decode(targets[:target_lengths[0]].tolist())
            
            print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.4f}")
            print(f" -> ì •ë‹µ: {target_str}")
            print(f" -> ì˜ˆì¸¡: {pred_str}")
            print("-" * 30)

debug_one_batch()