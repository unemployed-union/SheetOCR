# train_rl.py
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.distributions import Categorical
import pandas as pd
from tqdm import tqdm

# ê¸°ì¡´ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from .dataset import SheetMusicDataset, collate_fn
from .tokenizer import Tokenizer
from .vis_transformer import SimpleViTForOCR

import Levenshtein


def compute_reward(pred_text, target_text):
    """
    Levenshtein ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜
    - 1.0: ì™„ë²½í•˜ê²Œ ì¼ì¹˜
    - 0.0: í•˜ë‚˜ë„ ì•ˆ ë§ìŒ
    """
    if len(target_text) == 0:
        return 0.0
    
    # í¸ì§‘ ê±°ë¦¬ ê³„ì‚° (ëª‡ ê¸€ìë¥¼ ê³ ì³ì•¼ ì •ë‹µì´ ë˜ëŠ”ì§€)
    distance = Levenshtein.distance(pred_text, target_text)
    max_len = max(len(pred_text), len(target_text))
    
    # ì ìˆ˜í™” (0 ~ 1 ì‚¬ì´ë¡œ ì •ê·œí™”)
    # ê±°ë¦¬ê°€ 0ì´ë©´ scoreëŠ” 1.0 (ìµœê³ )
    score = 1.0 - (distance / max_len)
    
    return score


def train_rl_epoch(model, dataloader, optimizer, device, tokenizer):
    model.train()
    total_reward = 0
    total_loss = 0
    
    progress_bar = tqdm(dataloader, desc="ğŸš€ RL Training")
    
    for images, targets, target_lengths in progress_bar:
        images = images.to(device)
        batch_size = images.size(0)
        
        # 1. ëª¨ë¸ ì˜ˆì¸¡ (Logits ì¶”ì¶œ)
        # logits shape: [Batch, SeqLen, Vocab]
        logits = model(images)
        
        # -------------------------------------------------------
        # [í•µì‹¬] SCST (Self-Critical Sequence Training) ì•Œê³ ë¦¬ì¦˜
        # -------------------------------------------------------
        
        # A. í™•ë¥  ë¶„í¬ ë§Œë“¤ê¸°
        probs = torch.softmax(logits, dim=-1)
        dist = Categorical(probs)
        
        # B. ë‘ ê°€ì§€ ë²„ì „ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
        # (1) Sampling: í™•ë¥ ì— ë”°ë¼ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„ë´„ (ëª¨í—˜)
        sample_ids = dist.sample() 
        
        # (2) Greedy: í™•ë¥ ì´ ì œì¼ ë†’ì€ ê²ƒë§Œ ë½‘ìŒ (Baseline/ê¸°ì¤€ì )
        with torch.no_grad():
            greedy_ids = probs.argmax(dim=-1)
            
        # C. ë³´ìƒ(Reward) ê³„ì‚°
        # ë°°ì¹˜ ë‚´ì˜ ê° ìƒ˜í”Œë§ˆë‹¤ ë³´ìƒ ê³„ì‚°
        rl_loss = 0
        batch_avg_reward = 0
        
        for i in range(batch_size):
            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            sample_seq = sample_ids[i].tolist()
            greedy_seq = greedy_ids[i].tolist()
            
            # ì •ë‹µ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            # targetsëŠ” 1ì°¨ì›ìœ¼ë¡œ í´ì ¸ ìˆìœ¼ë¯€ë¡œ ì˜ë¼ë‚´ì•¼ í•¨ (collate_fn êµ¬ì¡°ìƒ ë³µì¡í•´ì„œ idx_to_charë¡œ ì§ì ‘ ë³€í™˜ ì¶”ì²œ)
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ dataloaderê°€ target_textë¥¼ ì£¼ë©´ ì¢‹ì§€ë§Œ, ì—†ìœ¼ë¯€ë¡œ targets í…ì„œì—ì„œ ë³µì›
            start = sum(target_lengths[:i])
            end = start + target_lengths[i]
            target_seq = targets[start:end].tolist()
            
            pred_text_sample = tokenizer.decode(sample_seq)
            pred_text_greedy = tokenizer.decode(greedy_seq)
            target_text = "".join([tokenizer.idx_to_char[idx] for idx in target_seq])
            
            # ì ìˆ˜ ë§¤ê¸°ê¸°
            reward_sample = compute_reward(pred_text_sample, target_text)
            reward_greedy = compute_reward(pred_text_greedy, target_text)
            
            # [ì¤‘ìš”] Advantage (ì´ë“) ê³„ì‚°
            # ë‚´ê°€ ëª¨í—˜(Sample)ì„ í•´ì„œ ê¸°ì¤€ì (Greedy)ë³´ë‹¤ ì–¼ë§ˆë‚˜ ì˜í–ˆë‚˜?
            advantage = reward_sample - reward_greedy
            
            # D. Loss ê³„ì‚° (Policy Gradient)
            # Log Probability * Advantage
            # ì˜í–ˆìœ¼ë©´(Adv > 0) ê·¸ í–‰ë™ì˜ í™•ë¥ ì„ ë†’ì´ê³ , ëª»í–ˆìœ¼ë©´(Adv < 0) ë‚®ì¶¤
            log_prob = dist.log_prob(sample_ids[i]).sum()
            rl_loss -= log_prob * advantage  # Gradient Descentë¥¼ ìœ„í•´ (-) ë¶™ì„
            
            batch_avg_reward += reward_sample

        # 2. ì—­ì „íŒŒ (ë°°ì¹˜ í‰ê· )
        optimizer.zero_grad()
        (rl_loss / batch_size).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += rl_loss.item()
        total_reward += batch_avg_reward / batch_size
        
        progress_bar.set_postfix({
            "Loss": f"{total_loss / (progress_bar.n + 1):.4f}", 
            "Reward": f"{total_reward / (progress_bar.n + 1):.4f}" # 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
        })

    return total_loss / len(dataloader), total_reward / len(dataloader)

def main():
    # --- ì„¤ì • ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    BATCH_SIZE = 32 # RLì€ ë©”ëª¨ë¦¬ ë§ì´ ë¨¹ìœ¼ë‹ˆ ë°°ì¹˜ë¥¼ ì¢€ ì¤„ì´ì„¸ìš”
    LR = 5e-6       # [ì¤‘ìš”] RLì€ í•™ìŠµë¥ ì„ ì•„ì£¼ì•„ì£¼ ë‚®ê²Œ ì¡ì•„ì•¼ í•©ë‹ˆë‹¤! (Supervisedì˜ 1/100)
    EPOCHS = 20
    
    # 1. Vocab & Tokenizer
    with open("vocab.txt", "r", encoding="utf-8") as f:
        vocab_list = [line.replace('\n', '').replace('\r', '') for line in f.readlines()]
    tokenizer = Tokenizer(vocab_list)
    
    # 2. Dataset
    df = pd.read_json("dataset_vit/train_final/metadata.jsonl", lines=True)

    rl_transform = transforms.Compose([
        transforms.Resize((112, 448)),        # í…ì„œë¡œ ë³€í™˜ (0~1)
        transforms.Normalize(mean=[0.5], std=[0.5]) # [í•µì‹¬] -1~1ë¡œ ì •ê·œí™”
    ])

    # RL í•  ë•ŒëŠ” Augmentationì„ ë„ê±°ë‚˜ ì•½í•˜ê²Œ í•˜ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤ (ì •ë‹µì„ í™•ì‹¤íˆ ë§ì¶”ëŠ” ê²Œ ëª©í‘œ)
    dataset = SheetMusicDataset("dataset_vit/train_final", df, tokenizer, transform=rl_transform)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    
    # 3. ëª¨ë¸ ë¡œë“œ
    model = SimpleViTForOCR(
        vocab_size=tokenizer.get_vocab_size(),
        embed_dim=384, num_layers=6 # ê¸°ì¡´ ì„¤ì • ìœ ì§€
    ).to(DEVICE)
    
    # [í•„ìˆ˜] ì§€ë„ í•™ìŠµ(Supervised)ìœ¼ë¡œ ë˜‘ë˜‘í•´ì§„ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    # 70% ì •í™•ë„ ì°ì€ ê·¸ íŒŒì¼ ê²½ë¡œë¥¼ ë„£ìœ¼ì„¸ìš”!
    pretrained_path = "best_model_one.pth" # íŒŒì¼ëª… í™•ì¸ í•„ìˆ˜!
    print(f"ğŸ”„ Pretrained Model ë¡œë“œ ì‹œë„: {pretrained_path}")
    
    # 1. ì¼ë‹¨ ë¶ˆëŸ¬ì˜¤ê¸°
    state_dict = torch.load(pretrained_path, map_location=DEVICE)
    
    # 2. 'module.' ì ‘ë‘ì‚¬ ì œê±° (DataParallelë¡œ ì €ì¥ëœ ê²½ìš° ëŒ€ë¹„)
    new_state_dict = {}
    for k, v in state_dict.items():
        name = k.replace("module.", "") # module. ì œê±°
        new_state_dict[name] = v
        
    # 3. ëª¨ë¸ì— ë„£ê¸° (strict=Trueë¡œ ë³€ê²½í•´ì„œ ì•ˆ ë§ìœ¼ë©´ ì—ëŸ¬ ë‚˜ê²Œ í•¨!)
    try:
        model.load_state_dict(new_state_dict, strict=True)
        print("âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì™„ë²½í•˜ê²Œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except Exception as e:
        print(f"ğŸš¨ [ì¹˜ëª…ì  ì˜¤ë¥˜] ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨! ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        # ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´, main.pyì˜ ëª¨ë¸ ì„¤ì •(ì¸µìˆ˜, íˆë“ ì‚¬ì´ì¦ˆ ë“±)ê³¼ 
        # train_rl.pyì˜ ëª¨ë¸ ì„¤ì •ì´ ë˜‘ê°™ì€ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
        exit()
    
    # 4. Optimizer
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    
    print("ğŸ”¥ ê°•í™” í•™ìŠµ(RL) ì‹œì‘! (ëª©í‘œ: Reward 1.0)")
    
    for epoch in range(EPOCHS):
        loss, reward = train_rl_epoch(model, dataloader, optimizer, DEVICE, tokenizer)
        
        print(f"Epoch [{epoch+1}/{EPOCHS}] RL Loss: {loss:.4f} | Avg Reward: {reward:.4f}")
        
        # ë³´ìƒì´ ë†’ì„ ë•Œ ì €ì¥
        if reward > 0.90:
            torch.save(model.state_dict(), f"rl_model_epoch_{epoch+1}_rew_{reward:.2f}.pth")

if __name__ == "__main__":
    main()