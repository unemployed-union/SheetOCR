import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ë¡œë”©ìš© ì¶”ê°€
import torch
import os
from torch import GradScaler
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR
from torchvision import transforms
from tqdm import tqdm

# ì§ì ‘ ë§Œë“  ëª¨ë“ˆë“¤ import
from .tokenizer import Tokenizer
from .vis_transformer import SimpleViTForOCR  # ì§ì ‘ ì§  ì»¤ìŠ¤í…€ ëª¨ë¸
from .dataset import SheetMusicDataset, collate_fn


def train(model, dataloader, criterion, optimizer, device, tokenizer, scheduler=None):
    model.train()
    epoch_loss = 0
    progress_bar = tqdm(dataloader, desc="Training")

    scaler = torch.amp.GradScaler('cuda')  # AMP ì‚¬ìš© (í•„ìˆ˜)

    # [ì„¤ì •] ì‹¤ì œë¡œëŠ” 16ê°œì”© ë„£ì§€ë§Œ, 4ë²ˆ ëª¨ì•„ì„œ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ 64ê°œ íš¨ê³¼
    accumulation_steps = 4

    optimizer.zero_grad()  # ë£¨í”„ ì‹œì‘ ì „ ì´ˆê¸°í™”

    for idx, (images, targets, target_lengths) in enumerate(progress_bar):
        images = images.to(device)
        targets = targets.to(device)
        target_lengths = target_lengths.to(device)

        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(images)
            outputs = outputs.permute(1, 0, 2)
            log_probs = nn.functional.log_softmax(outputs, dim=2)
            input_lengths = torch.full(size=(images.size(0),), fill_value=outputs.size(
                0), dtype=torch.long).to(device)

            loss = criterion(log_probs, targets, input_lengths, target_lengths)

            # [í•µì‹¬ 1] Lossë¥¼ ë‚˜ëˆ„ê¸° (4ë²ˆ ë”í•  ê±°ë‹ˆê¹Œ ë¯¸ë¦¬ 1/4ë¡œ ë‚˜ëˆ”)
            loss = loss / accumulation_steps

        # Backward (ê¸°ìš¸ê¸° ê³„ì‚°ë§Œ í•˜ê³  ì—…ë°ì´íŠ¸ëŠ” ì•„ì§ ì•ˆ í•¨)
        scaler.scale(loss).backward()

        # [ìˆ˜ì •] 4ë²ˆì§¸ ë°°ì¹˜ê±°ë‚˜, í˜¹ì€ 'ë§ˆì§€ë§‰' ë°°ì¹˜ë¼ë©´ ì—…ë°ì´íŠ¸!
        if (idx + 1) % accumulation_steps == 0 or (idx + 1) == len(dataloader):
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        # ë¡œê¹…ìš©ìœ¼ë¡œëŠ” ë‹¤ì‹œ ê³±í•´ì„œ ì›ë˜ loss ê°’ì„ ë³´ì—¬ì¤Œ
        current_loss = loss.item() * accumulation_steps
        epoch_loss += current_loss
        progress_bar.set_postfix({"Loss": current_loss})

    return epoch_loss / len(dataloader)


def evaluate(model, dataloader, criterion, device, tokenizer):
    model.eval()
    total_loss = 0
    correct_count = 0
    total_count = 0
    sample_count = 0

    with torch.no_grad():
        for images, targets, target_lengths in dataloader:
            images = images.to(device)
            targets = targets.to(device)
            target_lengths = target_lengths.to(device)

            outputs = model(images)

            # Loss ê³„ì‚°ìš©
            outputs_loss = outputs.permute(1, 0, 2)
            log_probs = nn.functional.log_softmax(outputs_loss, dim=2)
            input_lengths = torch.full(
                size=(images.size(0),), fill_value=outputs.size(1), dtype=torch.long
            ).to(device)

            loss = criterion(log_probs.cpu(), targets.cpu(),
                             input_lengths.cpu(), target_lengths.cpu())
            total_loss += loss.item()

            # ì •í™•ë„ ê³„ì‚°ìš© (Greedy Decoding)
            pred_indices = outputs.argmax(dim=2)

            current_target_idx = 0
            batch_size = images.size(0)

            for i in range(batch_size):
                # ì˜ˆì¸¡ê°’ ë¬¸ìì—´ë¡œ ë³€í™˜
                pred_seq = pred_indices[i].tolist()
                # ì¤‘ë³µ ì œê±° ë¡œì§ì€ tokenizer ì•ˆì— ìˆë‹¤ê³  ê°€ì •
                pred_text = tokenizer.decode(pred_seq)

                # ì •ë‹µ ë¬¸ìì—´ë¡œ ë³€í™˜
                t_len = target_lengths[i].item()
                target_seq = targets[current_target_idx:
                                     current_target_idx + t_len].tolist()
                current_target_idx += t_len

                # ì •ë‹µì€ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ (idx_to_char ì´ìš©)
                target_text = "".join([tokenizer.idx_to_char[idx]
                                      for idx in target_seq])

                if pred_text == target_text:
                    correct_count += 1
                total_count += 1

                if sample_count < 2:  # ì—í­ë‹¹ 2ê°œë§Œ ìƒ˜í”Œ ì¶œë ¥
                    print(
                        f"   [ê²€ì¦] ì •ë‹µ: {target_text[:20]:<20} | ì˜ˆì¸¡: {pred_text[:20]}")
                    sample_count += 1

    avg_loss = total_loss / len(dataloader)
    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0.0

    return avg_loss, accuracy


def main():
    # --- [ì„¤ì •] ---
    BATCH_SIZE = 64        # RAM ìºì‹±í–ˆìœ¼ë‹ˆ 64ë„ ê±°ëœ¬í•¨ (ì•ˆë˜ë©´ 32ë¡œ ì¤„ì´ê¸°)
    LEARNING_RATE = 1e-4   # 1e-4 -> 2e-4 (ë°°ì¹˜ ëŠ˜ë ¸ìœ¼ë‹ˆ ì¡°ê¸ˆ ì˜¬ë¦¼)
    EPOCHS = 100         # ë„‰ë„‰í•˜ê²Œ ì¡ê³  Early Stopping í•˜ì„¸ìš”
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available():
        DEVICE = "mps"  # Macìš©

    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True

    # 1. Tokenizer ë¡œë“œ
    vocab_list = []

        # [ìˆ˜ì • ì½”ë“œ] ì—”í„°ì™€ ë¦¬í„´ë§Œ ì œê±°í•˜ê³ , ìŠ¤í˜ì´ìŠ¤ë°”ëŠ” ì‚´ë ¤ë‘¡ë‹ˆë‹¤.
    with open("vocab.txt", "r", encoding="utf-8") as f:
        vocab_list = [line.replace('\n', '').replace('\r', '')
                    for line in f.readlines()]

    # [í™•ì¸ ì‚¬ì‚´ìš© ì½”ë“œ - ì‹¤í–‰ ì‹œ ì½˜ì†”ì— ëœ¸]
    if ' ' in vocab_list:
        print(f"âœ… Vocab ë¡œë“œ ì„±ê³µ! ìŠ¤í˜ì´ìŠ¤ë°”ê°€ {vocab_list.index(' ')}ë²ˆ ì¸ë±ìŠ¤ì— ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸš¨ ë¹„ìƒ! ì—¬ì „íˆ ìŠ¤í˜ì´ìŠ¤ë°”ê°€ Vocab ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
        
    tokenizer = Tokenizer(vocab_list)

    # 2. ë°ì´í„°ì…‹ ì¤€ë¹„ (Pandasë¡œ ë¨¼ì € ì½ê¸°)
    # transform = transforms.Normalize(mean=[0.5], std=[0.5])
    train_transform = transforms.Compose([
        # í™•ë¥ (p)ì„ 0.5 -> 0.3ìœ¼ë¡œ ë‚®ì¶¤ (ì¼ë‹¨ ì‰¬ìš´ ê±° ë§ì´ ë³´ê³  ë°°ìš°ë¼ê³ )
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=(3, 5), sigma=(0.1, 1.5))
        ], p=0.5),  # 30% í™•ë¥ ë¡œë§Œ íë¦¬ê²Œ

        # ë°ê¸° ë³€í™”ë„ ì¡°ê¸ˆ ì•½í•˜ê²Œ
        transforms.ColorJitter(brightness=0.1, contrast=0.1),

        transforms.RandomApply([
            transforms.RandomAffine(
                degrees=2,              # íšŒì „ ê°ë„ ì¤„ì„ (3 -> 2)
                translate=(0.02, 0.02),  # ì´ë™ ë²”ìœ„ ì¤„ì„
                scale=(0.99, 1.02),
                fill=0
            )
        ], p=0.5),  # 30% í™•ë¥ ë¡œë§Œ ë¹„í‹€ê¸°

        transforms.Normalize(mean=[0.5], std=[0.5]),
    ])

    # ê²€ì¦ìš©: ê¹¨ë—í•˜ê²Œ ì •ê·œí™”ë§Œ
    val_transform = transforms.Normalize(mean=[0.5], std=[0.5])

    # JSONL íŒŒì¼ì„ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    print("ğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_json("dataset_vit/train_final/metadata.jsonl", lines=True)

    df = df.sample(frac=1).reset_index(drop=True)  # ì „ì²´ ì…”í”Œ
    split_idx = int(0.9 * len(df))
    train_df = df.iloc[:split_idx]
    val_df = df.iloc[split_idx:]

    # í•™ìŠµ ë°ì´í„°ì…‹ (Augmentation ì ìš©!)
    train_dataset = SheetMusicDataset(
        root_dir="dataset_vit/train_final",
        df=train_df,
        tokenizer=tokenizer,
        transform=train_transform  # <-- ì—¬ê¸°ì— train_transform ì ìš©
    )

    # ê²€ì¦ ë°ì´í„°ì…‹ (ê¹¨ë—í•¨)
    val_dataset = SheetMusicDataset(
        root_dir="dataset_vit/train_final",
        df=val_df,
        tokenizer=tokenizer,
        transform=val_transform    # <-- ì—¬ê¸°ì— val_transform ì ìš©
    )

    # DataLoader (RAM ìºì‹±ì„ ì¼ìœ¼ë¯€ë¡œ num_workersëŠ” ì ì–´ë„ ë¨)
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4,      # 0 -> 4 (ë˜ëŠ” 8) ë³€ê²½! (CPUê°€ ë³‘ë ¬ë¡œ ë°ì´í„° ì¤€ë¹„)
        pin_memory=True     # False -> True ë³€ê²½! (GPU ì „ì†¡ ê°€ì†)
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=4,      # ì—¬ê¸°ë„ ë˜‘ê°™ì´
        pin_memory=True     # ì—¬ê¸°ë„ ë˜‘ê°™ì´
    )

    # 3. ëª¨ë¸ ì´ˆê¸°í™” (Custom ViT)
    # ì§ì ‘ êµ¬í˜„í•œ SimpleViTForOCR ì‚¬ìš©
    model = SimpleViTForOCR(
        vocab_size=tokenizer.get_vocab_size(),
        embed_dim=384,   # ViT Smallê¸‰
        # num_heads=6,     # 384 / 64 = 6
        # num_layers=12     # ë ˆì´ì–´ 6ê°œ (ê³µë¶€ìš©ìœ¼ë¡œ ì ë‹¹)
    ).to(DEVICE)

    load_path = "final_model.pth" # ì˜ ëë˜ ê·¸ íŒŒì¼

    if os.path.exists(load_path):
        print(f"ğŸ”¥ {load_path} ë¡œë“œ! 70%ì—ì„œ ë‹¤ì‹œ ë“±ë°˜ ì‹œì‘!")
        # strict=Trueë¡œ í•´ì„œ í™•ì‹¤í•˜ê²Œ ë¡œë“œ (êµ¬ì¡° ì•ˆ ë°”ê¿¨ìœ¼ë‹ˆê¹Œìš”)
        model.load_state_dict(torch.load(load_path), strict=True)

    criterion = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.AdamW(
        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # 4. ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (verbose ì‚­ì œ, mode='max' í™•ì¸)
    # ì •í™•ë„(Acc)ê°€ ì•ˆ ì˜¤ë¥´ë©´ LRì„ ê¹ìŠµë‹ˆë‹¤.
    # scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=5,      # 3ì€ ë„ˆë¬´ ê¸‰í•¨. 5ë²ˆ ì •ë„ëŠ” ì°¸ì•„ì£¼ê²Œ ë³€ê²½
        min_lr=1e-6     # [ì¤‘ìš”] ì•„ë¬´ë¦¬ ê¹ì•„ë„ 0.000001 ë°‘ìœ¼ë¡œëŠ” ì•ˆ ë‚´ë ¤ê°!
    )

    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘! (Device: {DEVICE})")

    # --- [í•™ìŠµ ë£¨í”„] ---
    for epoch in range(EPOCHS):
        train_loss = train(model, train_loader, criterion,
                           optimizer, DEVICE, tokenizer, scheduler=scheduler)
        val_loss, val_acc = evaluate(
            model, val_loader, criterion, DEVICE, tokenizer)

        # [ì¤‘ìš”] ìŠ¤ì¼€ì¤„ëŸ¬ì—ê²Œ ì •í™•ë„ë¥¼ ì•Œë ¤ì¤Œ
        # scheduler.step(val_acc) # OneCycleLRì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì£¼ì„ì²˜ë¦¬

        # í˜„ì¬ LR ì°ì–´ë³´ê¸°
        current_lr = optimizer.param_groups[0]['lr']

        print(f"Epoch [{epoch+1}/{EPOCHS}] "
              f"Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Acc: {val_acc:.2f}% | "
              f"LR: {current_lr:.8f}")

        # ëª¨ë¸ ì €ì¥ (ì •í™•ë„ ì˜¤ë¥¼ ë•Œë§Œ ì €ì¥í•˜ëŠ” ë¡œì§ ì¶”ê°€í•˜ë©´ ë” ì¢‹ìŒ)
        if val_acc > 80:  # 80% ë„˜ìœ¼ë©´ ì €ì¥ ì‹œì‘
            torch.save(model.state_dict(),
                       f"model_epoch_{epoch+1}_acc_{val_acc:.1f}.pth")

        scheduler.step(val_acc)

    # ìµœì¢… ì €ì¥
    torch.save(model.state_dict(), "final_model.pth")


if __name__ == "__main__":
    main()
