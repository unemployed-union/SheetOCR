import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ë¡œë”©ìš© ì¶”ê°€
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR
from torchvision import transforms
from tqdm import tqdm

# ì§ì ‘ ë§Œë“  ëª¨ë“ˆë“¤ import
from .tokenizer import Tokenizer
from .vis_transformer import SimpleViTForOCR  # ì§ì ‘ ì§  ì»¤ìŠ¤í…€ ëª¨ë¸
from .dataset import SheetMusicDataset, collate_fn

# ìƒë‹¨ import ì¶”ê°€
from torch import amp

def train(model, dataloader, criterion, optimizer, device, tokenizer, scheduler=None):
    model.train()
    epoch_loss = 0
    progress_bar = tqdm(dataloader, desc="Training")
    
    # [ì¶”ê°€] GradScalerëŠ” CUDAìš©ì´ë¼ MPSì—ì„œëŠ” ë³´í†µ ì•ˆ ì¨ë„ ë˜ì§€ë§Œ, 
    # PyTorch ìµœì‹  ë²„ì „ì—ì„œëŠ” MPSë„ scalerë¥¼ ì§€ì›í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. 
    # ì•ˆì „í•˜ê²Œ autocastë§Œ ë¨¼ì € ì ìš©í•´ë´…ë‹ˆë‹¤.

    for images, targets, target_lengths in progress_bar:
        images = images.to(device)
        targets = targets.to(device)
        target_lengths = target_lengths.to(device)

        optimizer.zero_grad()

        # [í•µì‹¬] Autocast ì ìš© (MPS ëª¨ë“œ)
        # ì—°ì‚°ì„ Float16ìœ¼ë¡œ ì••ì¶•í•´ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        with amp.autocast(device_type="mps", dtype=torch.float16):
            outputs = model(images)
            outputs = outputs.permute(1, 0, 2)
            log_probs = nn.functional.log_softmax(outputs, dim=2)
            input_lengths = torch.full(size=(images.size(0),), fill_value=outputs.size(0), dtype=torch.long).to(device)
            
            loss = criterion(log_probs.cpu(), targets.cpu(), input_lengths.cpu(), target_lengths.cpu())

        # ì—­ì „íŒŒ
        loss.backward()
        
        # Gradient Clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        
        optimizer.step()
        if scheduler:
            scheduler.step()

        epoch_loss += loss.item()
        progress_bar.set_postfix({"Loss": loss.item()})

    return epoch_loss / len(dataloader)


def evaluate(model, dataloader, criterion, device, tokenizer):
    model.eval()
    total_loss = 0
    correct_count = 0
    total_count = 0
    sample_count = 0

    with torch.no_grad():
        for images, targets, target_lengths in dataloader:
            images = images.to(device)
            targets = targets.to(device)
            target_lengths = target_lengths.to(device)

            outputs = model(images)

            # Loss ê³„ì‚°ìš©
            outputs_loss = outputs.permute(1, 0, 2)
            log_probs = nn.functional.log_softmax(outputs_loss, dim=2)
            input_lengths = torch.full(
                size=(images.size(0),), fill_value=outputs.size(1), dtype=torch.long
            ).to(device)

            loss = criterion(log_probs.cpu(), targets.cpu(),
                             input_lengths.cpu(), target_lengths.cpu())
            total_loss += loss.item()

            # ì •í™•ë„ ê³„ì‚°ìš© (Greedy Decoding)
            pred_indices = outputs.argmax(dim=2)

            current_target_idx = 0
            batch_size = images.size(0)

            for i in range(batch_size):
                # ì˜ˆì¸¡ê°’ ë¬¸ìì—´ë¡œ ë³€í™˜
                pred_seq = pred_indices[i].tolist()
                # ì¤‘ë³µ ì œê±° ë¡œì§ì€ tokenizer ì•ˆì— ìˆë‹¤ê³  ê°€ì •
                pred_text = tokenizer.decode(pred_seq)

                # ì •ë‹µ ë¬¸ìì—´ë¡œ ë³€í™˜
                t_len = target_lengths[i].item()
                target_seq = targets[current_target_idx:
                                     current_target_idx + t_len].tolist()
                current_target_idx += t_len

                # ì •ë‹µì€ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ (idx_to_char ì´ìš©)
                target_text = "".join([tokenizer.idx_to_char[idx]
                                      for idx in target_seq])

                if pred_text == target_text:
                    correct_count += 1
                total_count += 1

                if sample_count < 2:  # ì—í­ë‹¹ 2ê°œë§Œ ìƒ˜í”Œ ì¶œë ¥
                    print(
                        f"   [ê²€ì¦] ì •ë‹µ: {target_text[:20]:<20} | ì˜ˆì¸¡: {pred_text[:20]}")
                    sample_count += 1

    avg_loss = total_loss / len(dataloader)
    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0.0

    return avg_loss, accuracy


def main():
    # --- [ì„¤ì •] ---
    BATCH_SIZE = 32        # RAM ìºì‹±í–ˆìœ¼ë‹ˆ 64ë„ ê±°ëœ¬í•¨ (ì•ˆë˜ë©´ 32ë¡œ ì¤„ì´ê¸°)
    LEARNING_RATE = 5e-4   # 1e-4 -> 2e-4 (ë°°ì¹˜ ëŠ˜ë ¸ìœ¼ë‹ˆ ì¡°ê¸ˆ ì˜¬ë¦¼)
    EPOCHS = 80           # ë„‰ë„‰í•˜ê²Œ ì¡ê³  Early Stopping í•˜ì„¸ìš”
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available():
        DEVICE = "mps"  # Macìš©

    # 1. Tokenizer ë¡œë“œ
    vocab_list = []
    with open("vocab.txt", "r", encoding="utf-8") as f:
        vocab_list = [line.strip('\n') for line in f.readlines()]
    tokenizer = Tokenizer(vocab_list)

    # 2. ë°ì´í„°ì…‹ ì¤€ë¹„ (Pandasë¡œ ë¨¼ì € ì½ê¸°)
    transform = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])

    # JSONL íŒŒì¼ì„ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    print("ğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_json("dataset/train/metadata.jsonl", lines=True)

    # Dataset ìƒì„± (ì—¬ê¸°ì„œ RAM ìºì‹±ì´ ì¼ì–´ë‚¨ - ì‹œê°„ ì¢€ ê±¸ë¦¼)
    full_dataset = SheetMusicDataset(
        root_dir="dataset/train_resized",
        df=df,
        tokenizer=tokenizer,
        transform=transform
    )

    # Train/Val ë¶„ë¦¬
    train_size = int(0.9 * len(full_dataset))  # ê²€ì¦ ë°ì´í„° 10%ë§Œ
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size])

    # DataLoader (RAM ìºì‹±ì„ ì¼ìœ¼ë¯€ë¡œ num_workersëŠ” ì ì–´ë„ ë¨)
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        collate_fn=collate_fn, num_workers=0, pin_memory=False
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False,
        collate_fn=collate_fn, num_workers=0, pin_memory=False
    )

    # 3. ëª¨ë¸ ì´ˆê¸°í™” (Custom ViT)
    # ì§ì ‘ êµ¬í˜„í•œ SimpleViTForOCR ì‚¬ìš©
    model = SimpleViTForOCR(
        vocab_size=tokenizer.get_vocab_size(),
        embed_dim=384,   # ViT Smallê¸‰
        # num_heads=6,     # 384 / 64 = 6
        # num_layers=6     # ë ˆì´ì–´ 6ê°œ (ê³µë¶€ìš©ìœ¼ë¡œ ì ë‹¹)
    ).to(DEVICE)

    criterion = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.AdamW(
        model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # 4. ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (verbose ì‚­ì œ, mode='max' í™•ì¸)
    # ì •í™•ë„(Acc)ê°€ ì•ˆ ì˜¤ë¥´ë©´ LRì„ ê¹ìŠµë‹ˆë‹¤.
    # scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

    scheduler = OneCycleLR(
        optimizer,
        max_lr=1e-3,              # ìµœëŒ€ í•™ìŠµë¥  (ì—¬ê¸°ê¹Œì§€ ì°ê³  ë‚´ë ¤ì˜´)
        epochs=EPOCHS,            # ì „ì²´ ì—í­ ìˆ˜
        steps_per_epoch=len(train_loader),
        pct_start=0.1,            # ì „ì²´ ê³¼ì •ì˜ ì•ë¶€ë¶„ 10% ë™ì•ˆ LRì„ ì˜¬ë¦¼ (Warm-up)
        anneal_strategy='cos'     # ì½”ì‚¬ì¸ ê³¡ì„ ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ
    )

    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘! (Device: {DEVICE})")

    # --- [í•™ìŠµ ë£¨í”„] ---
    for epoch in range(EPOCHS):
        train_loss = train(model, train_loader, criterion,
                           optimizer, DEVICE, tokenizer)
        val_loss, val_acc = evaluate(
            model, val_loader, criterion, DEVICE, tokenizer)

        # [ì¤‘ìš”] ìŠ¤ì¼€ì¤„ëŸ¬ì—ê²Œ ì •í™•ë„ë¥¼ ì•Œë ¤ì¤Œ
        scheduler.step(val_acc)

        # í˜„ì¬ LR ì°ì–´ë³´ê¸°
        current_lr = optimizer.param_groups[0]['lr']

        print(f"Epoch [{epoch+1}/{EPOCHS}] "
              f"Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Acc: {val_acc:.2f}% | "
              f"LR: {current_lr:.8f}")

        # ëª¨ë¸ ì €ì¥ (ì •í™•ë„ ì˜¤ë¥¼ ë•Œë§Œ ì €ì¥í•˜ëŠ” ë¡œì§ ì¶”ê°€í•˜ë©´ ë” ì¢‹ìŒ)
        if val_acc > 80:  # 80% ë„˜ìœ¼ë©´ ì €ì¥ ì‹œì‘
            torch.save(model.state_dict(),
                       f"model_epoch_{epoch+1}_acc_{val_acc:.1f}.pth")

    # ìµœì¢… ì €ì¥
    torch.save(model.state_dict(), "final_model.pth")


if __name__ == "__main__":
    main()
