import torch
from ultralytics import YOLO
from PIL import Image
from torchvision import transforms

# ìš°ë¦¬ê°€ ë§Œë“  ViT ëª¨ë“ˆë“¤
from vis_transformer.tokenizer import Tokenizer
from vis_transformer.vis_transformer import SimpleViTForOCR

# -------------------------------------------------
# 1. ëª¨ë¸ ë¡œë”© (YOLO + ViT)
# -------------------------------------------------
def load_models(yolo_path, vit_path, vocab_file, device):
    # (A) YOLO ë¡œë“œ (Detector)
    print("ğŸ‘€ Loading Detector (YOLO)...")
    detector = YOLO(yolo_path)

    # (B) ViT ë¡œë“œ (Recognizer)
    print("ğŸ§  Loading Recognizer (ViT)...")
    with open(vocab_file, "r", encoding="utf-8") as f:
        vocab_list = [line.strip('\n') for line in f.readlines()]
    tokenizer = Tokenizer(vocab_list)
    
    recognizer = SimpleViTForOCR(
        vocab_size=len(vocab_list) + 1,
        img_height=112, img_width=448, embed_dim=768
        # í•™ìŠµ ë•Œ ì“´ íŒŒë¼ë¯¸í„° í™•ì¸ í•„ìˆ˜!
    )
    recognizer.load_state_dict(torch.load(vit_path, map_location=device))
    recognizer.to(device)
    recognizer.eval()

    return detector, recognizer, tokenizer

# -------------------------------------------------
# 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# -------------------------------------------------
def get_title_from_sheet(image_path, detector, recognizer, tokenizer, device):
    # (A) ì œëª© ìœ„ì¹˜ ì°¾ê¸° (Detection)
    # conf=0.25: í™•ì‹ ì´ 25% ì´ìƒì¸ ê²ƒë§Œ ì°¾ê¸°
    results = detector.predict(image_path, conf=0.25) 
    
    # ì°¾ì€ ê²Œ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if len(results[0].boxes) == 0:
        return "âŒ ì œëª©ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤."

    # ì—¬ëŸ¬ ê°œ ì°¾ì•˜ìœ¼ë©´, ê°€ì¥ ìœ„ì— ìˆëŠ” ë†ˆ(yì¢Œí‘œ ìµœì†Œ)ì´ ì œëª©ì¼ í™•ë¥  99%
    # box format: xyxy (x1, y1, x2, y2)
    boxes = results[0].boxes.xyxy.cpu().numpy()
    
    # y1(ì„¸ë¡œ ìœ„ì¹˜) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì„œ ë§¨ ìœ„ ë°•ìŠ¤ ì„ íƒ
    boxes = sorted(boxes, key=lambda x: x[1]) 
    best_box = boxes[0] 
    
    x1, y1, x2, y2 = map(int, best_box)
    
    # (B) ì´ë¯¸ì§€ ìë¥´ê¸° (Crop)
    original_img = Image.open(image_path).convert('RGB')
    
    # ì—¬ë°±(Padding)ì„ ì¢€ ì¤˜ì•¼ ê¸€ìê°€ ì•ˆ ì˜ë¦¬ê³  ViTê°€ ì˜ ì½ìŒ
    padding = 10
    w, h = original_img.size
    crop_box = (
        max(0, x1 - padding), 
        max(0, y1 - padding), 
        min(w, x2 + padding), 
        min(h, y2 + padding)
    )
    title_img = original_img.crop(crop_box)

    # (C) ê¸€ì ì½ê¸° (Recognition)
    # ViTìš© ì „ì²˜ë¦¬ (Resize & Normalize)
    transform = transforms.Compose([
        transforms.Resize((112, 448)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    input_tensor = transform(title_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        output = recognizer(input_tensor)
        pred_indices = output.argmax(dim=2)
        decoded_text = tokenizer.decode(pred_indices[0].tolist())

    return decoded_text, title_img

# -------------------------------------------------
# ì‹¤í–‰
# -------------------------------------------------
if __name__ == "__main__":
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # íŒŒì¼ ê²½ë¡œë“¤
    YOLO_PATH = "runs/detect/train/weights/best.pt" # YOLO í•™ìŠµ ê²°ê³¼
    VIT_PATH = "best_model.pth"                     # ViT í•™ìŠµ ê²°ê³¼
    VOCAB_PATH = "vocab.txt"
    TEST_IMG = "test_images/ì€í˜œ.jpg"

    # ë¡œë“œ
    detector, recognizer, tokenizer = load_models(YOLO_PATH, VIT_PATH, VOCAB_PATH, DEVICE)
    
    # ì‹¤í–‰
    title_text, cropped_img = get_title_from_sheet(TEST_IMG, detector, recognizer, tokenizer, DEVICE)
    
    print(f"ğŸµ ì¶”ì¶œëœ ì œëª©: {title_text}")
    
    # ì˜ë¦° ì´ë¯¸ì§€ í™•ì¸
    cropped_img.show()